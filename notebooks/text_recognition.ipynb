{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from lxml import etree\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from kiebids.modules.evaluation import process_xml_files, load_image_from_url, draw_polygon_on_image\n",
    "from kiebids import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download XML files \n",
    "# output_path = \"../data\"\n",
    "\n",
    "# process_xml_files(\n",
    "#    os.path.join(config.shared_folder, 'hymdata_sample/20230511T160908__coll.mfn-berlin.de_u_78a081'),\n",
    "#     output_path\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from tqdm import tqdm\n",
    "# from io import BytesIO\n",
    "# from PIL import Image, ImageDraw, ImageFont\n",
    "# import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = config.shared_folder + '/hymdata_sample/20230511T160908__coll.mfn-berlin.de_u_78a081'\n",
    "def get_root(folder_path, i): \n",
    "\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "    \n",
    "    file_path = os.path.join(folder_path, files[i])\n",
    "    tree = etree.parse(file_path)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_image(path, i, display_image=False):\n",
    "\n",
    "    root = get_root(path, i=i)\n",
    "    ns = {'ns': root.nsmap[None]} if None in root.nsmap else {}\n",
    "    comments = root.find('.//ns:Metadata/ns:Comments' if ns else './/Metadata/Comments', namespaces=ns)\n",
    "    # excluding some fields without assignment\n",
    "    comments = dict(item.split(\"=\", 1) for item in comments.text.split(\", \") if len(item.split(\"=\", 1))==2)\n",
    "\n",
    "    # loading from url  \n",
    "    image_url = comments.get('imgUrl')\n",
    "    image = None\n",
    "    if image_url:\n",
    "        image = load_image_from_url(image_url)\n",
    "\n",
    "    if display_image: \n",
    "        display(image)\n",
    "\n",
    "    return image\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(folder_path, output_path = \"../data/images\"):\n",
    "    files = [f for f in os.listdir(folder_path) if f.endswith('.xml')]\n",
    "\n",
    "    for file in files: \n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        tree = etree.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        ns = {'ns': root.nsmap[None]} if None in root.nsmap else {}\n",
    "        comments = root.find('.//ns:Metadata/ns:Comments' if ns else './/Metadata/Comments', namespaces=ns)\n",
    "        # excluding so#\n",
    "        comments = dict(item.split(\"=\", 1) for item in comments.text.split(\", \") if len(item.split(\"=\", 1))==2)\n",
    "        image_url = comments.get('imgUrl')\n",
    "        if image_url:\n",
    "            image = load_image_from_url(image_url)\n",
    "\n",
    "        image.save(f\"{output_path}/{file.replace('.xml', '.jpg')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ground_truth_text(path, image_num, display_image=False):\n",
    "    \n",
    "    root = get_root(path,image_num)\n",
    "    ns = {'ns': root.nsmap[None]} if None in root.nsmap else {}\n",
    "    comments = root.find('.//ns:Metadata/ns:Comments' if ns else './/Metadata/Comments', namespaces=ns)\n",
    "    # excluding some fields without assignment\n",
    "    comments = dict(item.split(\"=\", 1) for item in comments.text.split(\", \") if len(item.split(\"=\", 1))==2) \n",
    "    transcriptions = \"\"\n",
    "    textlines = root.xpath('//ns:TextLine' if ns else '//TextLine', namespaces=ns)\n",
    "    \n",
    "    if display_image==True:\n",
    "        image_url = comments.get('imgUrl')\n",
    "        image = None\n",
    "        if image_url:\n",
    "            image = load_image_from_url(image_url)  \n",
    "\n",
    "    for image_num, textline in enumerate(textlines): \n",
    "        coords = textline.find('ns:Coords' if ns else 'Coords', namespaces=ns)\n",
    "\n",
    "        if coords is not None and display_image is True:\n",
    "            # loading from url\n",
    "            points = coords.get('points')\n",
    "            x_values, y_values = zip(*[tuple(map(int, point.split(','))) for point in points.split()]) \n",
    "\n",
    "            x_min = min(x_values)\n",
    "            x_max = max(x_values)\n",
    "            y_min = min(y_values)\n",
    "            y_max = max(y_values)\n",
    "            # print(x_min, y_min, x_max, y_max)\n",
    "            cropped_image = image.crop((x_min, y_min, x_max, y_max))\n",
    "            display(cropped_image)\n",
    "\n",
    "        unicode_elem = textline.find('.//ns:Unicode' if ns else './/Unicode', namespaces=ns)\n",
    "        if unicode_elem is not None:\n",
    "            print(f\"{image_num+1}: {unicode_elem.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EasyOCR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the easyOCR functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import easyocr\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from PIL import ImageDraw \n",
    "\n",
    "gpu = torch.cuda.is_available()\n",
    "easyocr_reader = easyocr.Reader(['en'], gpu=gpu)\n",
    "\n",
    "\n",
    "# Convert RGB to BGR\n",
    "\n",
    "def easyOCR(PIL_image):\n",
    "    cv2_img = cv2.cvtColor(np.array(PIL_image), cv2.COLOR_RGB2BGR)\n",
    "    result = easyocr_reader.readtext(cv2_img)\n",
    "\n",
    "    bounding_box = []\n",
    "    text = []\n",
    "    prob = []\n",
    "\n",
    "    for (b, t, p) in result: \n",
    "        bounding_box.append(b)\n",
    "        text.append(t)\n",
    "        prob.append(p)\n",
    "    return text, bounding_box, prob\n",
    "\n",
    "def easyOCR_bounding_boxes(PIL_image, display_image=False): \n",
    "\n",
    "    cv2_image = cv2.cvtColor(np.array(PIL_image), cv2.COLOR_RGB2BGR)\n",
    "    output = easyocr_reader.readtext(cv2_image)\n",
    "    bounding_boxes = [box for (box, text, probability) in output]\n",
    "\n",
    "    if display_image: \n",
    "        draw_image = PIL_image.copy()\n",
    "        draw = ImageDraw.Draw(draw_image)\n",
    "\n",
    "        for coords in bounding_boxes: \n",
    "            for i, coord in enumerate(coords):\n",
    "                if i+1==len(coords): \n",
    "                    draw.line([tuple(coord), tuple(coords[0])], fill=\"green\", width=0)\n",
    "                else: \n",
    "                    draw.line([tuple(coord), tuple(coords[i+1])], fill=\"green\", width=0)\n",
    "        display(draw_image)\n",
    "\n",
    "    return output\n",
    "\n",
    "def crop_image(PIL_image, coords, display_image=False):\n",
    "\n",
    "    x_values, y_values = zip(*coords)\n",
    "    \n",
    "    x_min = min(x_values)\n",
    "    x_max = max(x_values)\n",
    "    y_min = min(y_values)\n",
    "    y_max = max(y_values)\n",
    "\n",
    "    cropped_image = PIL_image.crop((x_min, y_min, x_max, y_max))\n",
    "    if display_image: \n",
    "        display(cropped_image)\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TrOCR handwritten \n",
    "- Different models Handwritten  \n",
    "TrOCR-Small-IAM  \n",
    "TrOCR-Base-IAM  \n",
    "TrOCR-Large-IAM ('microsoft/trocr-large-handwritten') \n",
    "\n",
    "- Different models printed  \n",
    "TrOCR-Small-SROIE  \n",
    "TrOCR-Base-SROIE  \n",
    "TrOCR-Large-SROIE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "model_name = 'microsoft/trocr-large-handwritten'\n",
    "processor = TrOCRProcessor.from_pretrained(model_name)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    model_name\n",
    ").to(device)\n",
    "\n",
    "def TrOcr(image, processor, model):\n",
    "    \"\"\"\n",
    "    :param image: PIL Image.\n",
    "    :param processor: Huggingface OCR processor.\n",
    "    :param model: Huggingface OCR model.\n",
    " \n",
    " \n",
    "    Returns:\n",
    "        generated_text: the OCR'd text string.\n",
    "    \"\"\"\n",
    "    # We can directly perform OCR on cropped images.\n",
    "    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True) #[0]\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Boxes \n",
    "We use easyOCR to extract the bounding boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display image\n",
    "image_num = 25 # which image\n",
    "image = get_image(path, image_num, display_image=False)\n",
    "easyOCR_output = easyOCR_bounding_boxes(image, display_image=True)\n",
    "easyOCR_result = []\n",
    "TrOcr_result = []\n",
    "for (bb,text,prob) in easyOCR_output: \n",
    "    cropped_image = crop_image(image, bb, display_image=False)\n",
    "    easyOCR_result.append((text,prob))\n",
    "    trOCR_text = TrOcr(cropped_image, processor,model)\n",
    "    TrOcr_result.append(trOCR_text)\n",
    "\n",
    "print(\"----- EasyOcr result -----\")\n",
    "for i, result in enumerate(easyOCR_result): \n",
    "    print(f\"{i+1}: {result[0]}         prob: {result[1]}\")\n",
    "\n",
    "print(\"------ Tr OCR Result --------\")\n",
    "for i, result in enumerate(TrOcr_result): \n",
    "    print(f\"{i+1}:  {result}\")\n",
    "\n",
    "print(\"----- Ground truth text ------\")\n",
    "ground_truth_text(path, image_num, display_image=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tesseract - Not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract \n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"/home/jupyter-lova/miniconda3/pkgs/tesseract-5.3.1-he1868e8_0/bin/tesseract\" \n",
    "# TESSDATA_PREFIX = \"/home/jupyter-lova/miniconda3/pkgs/tesseract-5.3.1-he1868e8_0/bin/\" \n",
    "# r\"/home/jupyter-lova/miniconda3/pkgs/tesseract-5.3.1-he1868e8_0/bin/tesseract\"\n",
    "# r\"/home/jupyter-lova/miniconda3/envs/app-kiebids/bin/pytesseract\" \n",
    "# r\"/home/jupyter-lova/miniconda3/pkgs/tesseract-5.3.1-he1868e8_0/bin/tesseract\" \n",
    "\n",
    "\n",
    "\n",
    "def test_tesseract(image): \n",
    "    return pytesseract.image_to_string(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing tesseract \n",
    "import os\n",
    "for r,s,f in os.walk(\"/\"):\n",
    "    for image_num in f:\n",
    "        if \"tesseract\" in image_num:\n",
    "            print(os.path.join(r,image_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for r,s,f in os.walk(\"/\"):\n",
    "    for image_num in f:\n",
    "        if \"tessdata\" in image_num:\n",
    "            print(os.path.join(r,image_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytesseract.pytesseract.tesseract_cmd = \"/home/jupyter-lova/miniconda3/envs/app-kiebids/bin/pytesseract\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test TrOCR Handwritten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    'microsoft/trocr-small-printed'\n",
    ").to(device)\n",
    "\n",
    "image = get_image(path, 5, display_image=True)\n",
    "test = ocr(image, processor, model)\n",
    "\n",
    "def ocr(image, processor, model):\n",
    "    \"\"\"\n",
    "    :param image: PIL Image.\n",
    "    :param processor: Huggingface OCR processor.\n",
    "    :param model: Huggingface OCR model.\n",
    " \n",
    " \n",
    "    Returns:\n",
    "        generated_text: the OCR'd text string.\n",
    "    \"\"\"\n",
    "    # We can directly perform OCR on cropped images.\n",
    "    pixel_values = processor(image, return_tensors='pt').pixel_values.to(device)\n",
    "    generated_ids = model.generate(pixel_values)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\n",
    "    'microsoft/trocr-small-printed'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = get_image(path, 5, display_image=True)\n",
    "test = ocr(image, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
